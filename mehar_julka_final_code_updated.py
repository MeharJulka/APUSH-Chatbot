# -*- coding: utf-8 -*-
"""Mehar_Julka_Final_Code_UPDATED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qpSKzcB4gIuATX2JV_WGrVPG9XJ6Rhrq
"""

!pip install  langchain_community  langchain_huggingface  huggingface_hub transformers colorama torch pdfminer.six chromadb langchain_openai langchain-google-genai

import os
import click
from typing import List

from langchain_community.document_loaders import TextLoader, PDFMinerLoader, CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
# from langchain.vectorstores import Milvus
from langchain.docstore.document import Document
# from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

import torch

from google.colab import drive
drive.mount('/content/drive')

class LingoChatbotCreateDB:
    def __init__(self):
        # self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = 'cuda'
        print(f"Using device: {self.device}")
        self.SOURCE_DIRECTORY = "/content/drive/MyDrive/CHATBOX"
        self.CHROMA_PATH = "/content/CHROMA"
        self.collection_ = "SOCIAL"
        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base",
                                                model_kwargs={"device": self.device})

        self.vector_store = Chroma(persist_directory=self.CHROMA_PATH,
                                   embedding_function=self.embeddings, collection_name=self.collection_)
        print("Loaded Embedding model!!")

    def load_single_document(self, file_path: str) -> Document:
        # Loads a single document from a file path
        if file_path.endswith(".txt"):
            loader = TextLoader(file_path)
        elif file_path.endswith(".pdf"):
            loader = PDFMinerLoader(file_path)
        elif file_path.endswith(".csv"):
            loader = CSVLoader(file_path)
        return loader.load()[0]

    def load_documents(self, source_dir: str) -> List[Document]:
        # Loads all documents from source documents directory
        all_files = os.listdir(source_dir)
        return [self.load_single_document(f"{source_dir}/{file_path}") for file_path in all_files if
                file_path[-4:] in ['.txt', '.pdf', '.csv']]

    def createDB(self):
        print(f"Loading documents from {self.SOURCE_DIRECTORY}")
        documents = self.load_documents(self.SOURCE_DIRECTORY)
        # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=150)
        text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=30, separator=" ")
        # text_splitter = CharacterTextSplitter(chunk_size=0, chunk_overlap=0)
        texts = text_splitter.split_documents(documents)
        print(f"Loaded {len(documents)} documents from {self.SOURCE_DIRECTORY}")
        print(f"Split into {len(texts)} chunks of text")


        print("Indexing the Documents!!!")
        self.vector_store.add_documents(documents=texts)
        print("="*50)
        print("Indexing of the Document Completed")
        print("="*50)

if __name__ == "__main__":
    create_db = LingoChatbotCreateDB()
    create_db.createDB()

"""# **RETRIEVAL**


"""

from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from colorama import Fore
import json
import time
import warnings
import os
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
# from openai import OpenAI # Remove this import
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI # Import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from colorama import Fore
import time
import warnings
import os
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline

warnings.filterwarnings("ignore")


class SLAiAssist:
    def __init__(self):
        t1 = time.time()
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(Fore.LIGHTGREEN_EX + "=" * 50, "Initializing Chatbot API", "=" * 50)
        self.CHROMA_PATH = "/content/CHROMA"
        self.collection_ = "SOCIAL"

        self.model_id = "Qwen/Qwen3-0.6B"
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_id
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id
        )
        self.pipe = pipeline(
            task="text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device=0,
            repetition_penalty=1.1,
            max_new_tokens=4096,
            return_full_text=True
        )
        self.sl_llm = HuggingFacePipeline(pipeline=self.pipe)

        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base",
                                                model_kwargs={"device": self.device})

        self.db = Chroma(
            persist_directory=self.CHROMA_PATH,
            embedding_function=self.embeddings,
            collection_name=self.collection_
        )


        self.retriever = self.db.as_retriever()

        self.SLqa = RetrievalQA.from_chain_type(llm=self.sl_llm, chain_type="stuff", retriever=self.retriever,
                                                return_source_documents=True)
        t2 = time.time()
        print(f">> Initializing Time: ", round((t2 - t1), 3))
        print(f"Running on: {self.device}")
        print(Fore.LIGHTCYAN_EX + "|| Loaded Chatbot ||")

    def assist(self, query):
        t1 = time.time()
        print("Chatbot answering the question...")
        res = self.SLqa.invoke({"query": query}) # Use invoke instead of direct call
        t2 = time.time()
        print(res)
        print("Answer Time: ", round((t2 - t1), 3))
        answer = res['result']
        output = {"question": query, "answer": answer}
        print("Chatbot OUTPUT: ", output)
        print("-"*50)
        print("User Query: ", output['question'])
        print("Output: ", output['answer'])

        return output

Agenassist = SLAiAssist()

# query = "Any AP US History related question..."
query = "Describe an important battle of the Civil War."
Agenassist.assist(query)

"""# RETRIEVAL USING GEMINI MODEL"""

from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from colorama import Fore
import json
import time
import warnings
import os
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from colorama import Fore
import time
import warnings
import os
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_google_genai import ChatGoogleGenerativeAI
import torch


warnings.filterwarnings("ignore")

os.environ["GOOGLE_API_KEY"] = "AIzaSyAQASPZMDbaImd4Mi5qoQ1ZDbF1lhzEJyQ"


class SLAiAssist:
    def __init__(self):
        t1 = time.time()
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(Fore.LIGHTGREEN_EX + "=" * 50, "Initializing Chatbot API", "=" * 50)
        self.CHROMA_PATH = "/content/CHROMA"
        self.collection_ = "SOCIAL"

        self.sl_llm = ChatGoogleGenerativeAI(
                            model="gemini-2.5-flash",
                            temperature=0,
                            max_tokens=None,
                            timeout=None,
                            max_retries=2,
                        )

        self.embeddings = HuggingFaceEmbeddings(model_name="moka-ai/m3e-base",
                                                model_kwargs={"device": self.device})

        self.db = Chroma(
            persist_directory=self.CHROMA_PATH,
            embedding_function=self.embeddings,
            collection_name=self.collection_
        )


        self.retriever = self.db.as_retriever()

        self.SLqa = RetrievalQA.from_chain_type(llm=self.sl_llm, chain_type="stuff", retriever=self.retriever,
                                                return_source_documents=True)
        t2 = time.time()
        print(f">> Initializing Time: ", round((t2 - t1), 3))
        print(f"Running on: {self.device}")
        print(Fore.LIGHTCYAN_EX + "|| Loaded Chatbot ||")

    def assist(self, query):
        t1 = time.time()
        print("Chatbot answering the question...")
        res = self.SLqa.invoke({"query": query}) # Use invoke instead of direct call
        t2 = time.time()
        print(res)
        print("Answer Time: ", round((t2 - t1), 3))
        answer = res['result']
        output = {"question": query, "answer": answer}
        print("Chatbot OUTPUT: ", output)
        print("-"*50)
        print("User Query: ", output['question'])
        print("Output: ", output['answer'])

        return output

agent = SLAiAssist()

# query = "Any APUSH related question..."
query = "What was the Great Depression?"
agent.assist(query)

"""# RETRIEVAL THROUGH CUSTOM PROMPT (OUT OF CONTEXT HANDLING MECHANISM)"""

from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from colorama import Fore
import time
import warnings
import os
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate

warnings.filterwarnings("ignore")


class SLAiAssist:
    def __init__(self):
        t1 = time.time()
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(Fore.LIGHTGREEN_EX + "=" * 50, "Initializing Chatbot API", "=" * 50)
        self.CHROMA_PATH = "/content/CHROMA"
        self.collection_ = "SOCIAL"

        # === Qwen-4B LLM setup ===
        self.model_id = "Qwen/Qwen3-0.6B"
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_id
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id
        )
        self.pipe = pipeline(
            task="text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device=0,
            repetition_penalty=1.1,
            max_new_tokens=4096,
            return_full_text=True
        )
        self.sl_llm = HuggingFacePipeline(pipeline=self.pipe)

        # Retrieval prompt
        self.template = '''Use the following context to answer the users question. If user question found in the context, return the exact answer from the context and never provide correct answer from your knowledge. Give answer only from the context, never give answer from your knowledge even if you know the answer for out of context question and you must say only "I don't know" in the answer.

                        Context: {context}
                        Question: {question}

                        If answer not found in the context, only say "I don't know" and never provide explanation even though you know the explanation for the given out of context question.
                        Answer:'''
        self.PROMPT = PromptTemplate(
            template=self.template, input_variables=["context", "question"]
        )

        # Embeddings + Chroma
        self.embeddings = HuggingFaceEmbeddings(
            model_name="moka-ai/m3e-base",
            model_kwargs={"device": self.device}
        )
        self.db = Chroma(
            persist_directory=self.CHROMA_PATH,
            embedding_function=self.embeddings,
            collection_name=self.collection_
        )
        self.retriever = self.db.as_retriever()

        # QA chain
        self.SLqa = RetrievalQA.from_chain_type(
            llm=self.sl_llm,
            chain_type="stuff",
            retriever=self.retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": self.PROMPT}
        )

        t2 = time.time()
        print(f">> Initializing Time: {round((t2 - t1), 3)} seconds")
        print(f"Running on: {self.device}")
        print(Fore.LIGHTCYAN_EX + "|| Loaded Chatbot ||")

    def assist(self, query: str) -> dict:
        t1 = time.time()
        print("Chatbot answering the question...")
        res = self.SLqa.invoke({"query": query})
        t2 = time.time()
        print(res)
        print("Answer Time: ", round((t2 - t1), 3), "seconds")
        answer = res['result']
        output = {"question": query, "answer": answer}
        print("Chatbot OUTPUT: ", output)
        print("-"*50)
        print("User Query: ", output['question'])
        print("Output: ", output['answer'])
        return output

agent = SLAiAssist()

query = "What is the weather like today?"
agent.assist(query)